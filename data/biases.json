[
  {
    "id": "fundamental-attribution-error",
    "title": "Fundamental Attribution Error",
    "category": "social",
    "summary": "We judge others by their character, but ourselves by our circumstances. When someone cuts us off in traffic, we think they're rude; when we do it, we had a good reason.",
    "why": "Our brains have asymmetric information: we know our own context and pressures, but only see others' actions. This creates a systematic bias toward dispositional (character-based) explanations for others' behavior while favoring situational explanations for our own. The actor-observer asymmetry occurs because we have rich contextual information about our own behavior but only observe others' actions without their internal states, motivations, or circumstances.",
    "counter": "Before judging someone's actions, pause and ask: 'What circumstances might explain this behavior?' Assume good intent and consider external factors. When evaluating your own behavior, ask if you'd judge others the same way in similar circumstances. Practice perspective-taking by imagining the other person's situation and constraints.",
    "source": "core",
    "researchLevel": "established",
    "tips": [
      "Ask 'What circumstances might explain this behavior?' before judging",
      "Assume good intent and consider external factors first",
      "Apply the same standards to yourself as you do to others",
      "Remember: everyone is fighting a battle you know nothing about"
    ],
    "examples": [
      {
        "title": "Hurricane Katrina Response",
        "description": "After Hurricane Katrina in 2005, many people attributed looting to the 'criminal nature' of New Orleans residents, rather than recognizing the desperate circumstances of people without food, water, or medical supplies for days. When similar behavior occurred after other disasters in predominantly white communities, it was described as 'finding supplies' rather than looting.",
        "source": "Hurricane Katrina disaster response",
        "year": 2005,
        "category": "news"
      },
      {
        "title": "Performance Review Bias",
        "description": "A Stanford study found that managers attributed male employees' failures to external factors ('difficult client,' 'unrealistic deadline') but attributed women's failures to personal shortcomings ('needs to work on leadership skills'). Meanwhile, men's successes were seen as skill-based while women's were attributed to luck or team effort.",
        "source": "Stanford Gender Bias Research",
        "year": 2014,
        "category": "business"
      },
      {
        "title": "Customer Service Complaints",
        "description": "When your food delivery arrives late, you might think the driver is lazy or incompetent. But when you're the one delivering and running late, you know you're dealing with impossible traffic, multiple wrong addresses, and app glitches. The same action, different attributions based on whose perspective you have access to.",
        "category": "personal"
      }
    ],
    "references": [
      {
        "title": "The Actor and the Observer: Divergent Perceptions of the Causes of Behavior",
        "authors": "Jones, E. E., & Nisbett, R. E.",
        "year": 1971,
        "journal": "General Learning Press",
        "type": "book"
      },
      {
        "title": "Attribution Theory and Research",
        "authors": "Kelley, H. H.",
        "year": 1973,
        "journal": "Annual Review of Psychology",
        "type": "review"
      }
    ],
    "attribution": {
      "termCoinedBy": {
        "name": "Lee Ross",
        "year": 1977
      },
      "keyContributors": [
        "Edward E. Jones & Richard E. Nisbett (1971) - Actor-observer asymmetry",
        "Harold Kelley (1967) - Attribution theory foundations"
      ],
      "researchConfidence": "high",
      "notes": "The term 'fundamental attribution error' was coined by Lee Ross in 1977. The underlying phenomenon was documented by Jones & Nisbett in their 1971 work on actor-observer differences."
    }
  },
  {
    "id": "self-serving-bias",
    "title": "Self-Serving Bias",
    "category": "social",
    "summary": "We attribute our successes to our skills and character, but blame our failures on external circumstances. Won the game? I'm talented. Lost? The referee was biased.",
    "why": "This bias protects our self-esteem and maintains a positive self-image. Our brains naturally seek patterns that confirm we're competent and good, while deflecting threats to our ego by externalizing failures. This self-enhancement motive is fundamental to human psychology and helps maintain psychological well-being.",
    "counter": "Practice intellectual humility. When you succeed, acknowledge the role of luck, timing, and others' help. When you fail, honestly assess your own contribution before blaming external factors. Keep a 'luck journal' noting when circumstances helped you.",
    "source": "core",
    "researchLevel": "established",
    "tips": [
      "When you succeed, acknowledge the role of luck and others' help",
      "When you fail, honestly assess your own contribution first",
      "Keep a 'luck journal' noting when circumstances helped you",
      "Practice saying 'I was wrong' or 'I made a mistake' regularly"
    ],
    "examples": [
      {
        "title": "Enron Executive Blame",
        "description": "After Enron's collapse in 2001, executives blamed 'market conditions,' 'accounting rules,' and 'media scrutiny' for the company's failure. Yet during profitable years, these same leaders credited their 'innovative strategies' and 'bold leadership.' The structural problems that caused the collapse existed during both periods, but attributions shifted based on outcomes.",
        "source": "Enron scandal",
        "year": 2001,
        "category": "business"
      },
      {
        "title": "Sports Team Fandom",
        "description": "Studies of sports fans found they used 'we' when their team won ('We played great today!') but 'they' when their team lost ('They couldn't execute'). Fans psychologically distance themselves from failure while claiming ownership of success, despite having zero impact on the game's outcome.",
        "source": "Sports psychology research",
        "year": 1976,
        "category": "personal"
      },
      {
        "title": "Investment Success Stories",
        "description": "Investors attribute profitable trades to their 'skill in reading the market' and 'disciplined strategy,' but blame losses on 'unexpected news,' 'market manipulation,' or 'bad luck.' Studies show this bias causes investors to take excessive risks after wins and become overly conservative after losses, both poor strategies.",
        "category": "business"
      }
    ],
    "references": [
      {
        "title": "The Self-Serving Attributional Bias: Beyond Self-Presentation Theory",
        "authors": "Miller, D. T., & Ross, M.",
        "year": 1975,
        "journal": "Journal of Personality and Social Psychology",
        "type": "study"
      },
      {
        "title": "Self-Serving Biases in the Attribution of Causality: Fact or Fiction?",
        "authors": "Zuckerman, M.",
        "year": 1979,
        "journal": "Psychological Bulletin",
        "type": "review"
      }
    ],
    "attribution": {
      "termCoinedBy": null,
      "keyContributors": [
        "Dale T. Miller & Michael Ross (1975) - Early systematic research",
        "Miron Zuckerman (1979) - Comprehensive review"
      ],
      "researchConfidence": "high",
      "notes": "Well-documented effect with extensive research. Miller & Ross provided early systematic investigation, though the phenomenon was observed earlier in attribution research."
    }
  },
  {
    "id": "in-group-favoritism",
    "title": "In-Group Favoritism",
    "category": "social",
    "summary": "We favor people who are similar to us or belong to our groups. We give them more trust, resources, and benefit of the doubt while being more critical of outsiders.",
    "why": "Evolutionary psychology suggests this helped early humans survive by strengthening tribal bonds. Our brains quickly categorize people as 'us' or 'them,' triggering different emotional and cognitive responses that favor our group.",
    "counter": "Actively seek diverse perspectives and relationships. When making decisions, ask: 'Would I judge this differently if it came from an outsider?' Use blind evaluation processes when possible. Regularly interact with people from different backgrounds.",
    "source": "core",
    "tips": [
      "Actively seek relationships with people from different backgrounds",
      "Ask: 'Would I judge this differently if it came from an outsider?'",
      "Use blind evaluation processes when making decisions",
      "Challenge yourself to find common ground with 'outsiders'"
    ],
    "examples": [
      {
        "title": "Tech Industry Hiring",
        "description": "A 2017 study found that tech companies hiring through employee referrals (typically from similar backgrounds) resulted in 85% of new hires being the same race and gender as existing employees. When companies implemented blind resume screening, diversity increased by 30% with no decrease in candidate quality.",
        "source": "Tech diversity research",
        "year": 2017,
        "category": "business"
      },
      {
        "title": "Venture Capital Funding Gap",
        "description": "In 2023, companies founded by alumni of Stanford, Harvard, and MIT received 35% of all VC funding, despite representing less than 1% of startups. VCs were more likely to fund founders who went to their own universities, worked at the same companies, or shared similar backgrounds—even when pitch quality was controlled for.",
        "source": "PitchBook VC funding analysis",
        "year": 2023,
        "category": "business"
      },
      {
        "title": "Jury Decision Patterns",
        "description": "Research on jury decisions found that jurors rated defendants more credible and sympathetic when they shared the same race, regional accent, or socioeconomic background. In identical mock trials, conviction rates varied by up to 25% based solely on perceived group similarity between jurors and defendants.",
        "category": "historical"
      }
    ]
  },
  {
    "id": "bandwagon-effect",
    "title": "Bandwagon Effect",
    "category": "social",
    "summary": "We adopt beliefs and behaviors because many others do, regardless of underlying evidence. The more people believe something, the more we're inclined to believe it too.",
    "why": "Social proof is a powerful heuristic: if many people do something, it's probably safe or correct. This saved cognitive energy for our ancestors and reduced risk. Our brains are wired to conform to avoid social rejection.",
    "counter": "Before adopting a popular belief, ask: 'What's the actual evidence?' Distinguish between popularity and validity. Seek out contrarian viewpoints. Remember that truth isn't democratic—many people can be wrong together.",
    "source": "core",
    "tips": [
      "Before adopting popular beliefs, ask 'What's the actual evidence?'",
      "Distinguish between popularity and validity",
      "Seek out contrarian viewpoints intentionally",
      "Remember: truth isn't democratic—many can be wrong together"
    ],
    "examples": [
      {
        "title": "GameStop Stock Frenzy",
        "description": "In January 2021, millions of retail investors bought GameStop stock primarily because others were buying it, driving the price from $20 to $483 in two weeks. Most buyers admitted they didn't understand the company's fundamentals but joined because of social media momentum. When the trend reversed, many lost substantial money.",
        "source": "GameStop short squeeze",
        "year": 2021,
        "category": "business"
      },
      {
        "title": "Tulip Mania Bubble",
        "description": "In 1637 Netherlands, tulip bulb prices reached extraordinarily high levels—some bulbs selling for more than a house—because everyone was buying them as investments. People quit jobs to trade tulips. When confidence evaporated, prices crashed 99% in weeks, bankrupting thousands who bought in late simply because 'everyone was doing it.'",
        "source": "Dutch Tulip Mania",
        "year": 1637,
        "category": "historical"
      },
      {
        "title": "Restaurant Line Psychology",
        "description": "Studies show that restaurants with visible lines attract more customers, even when equal or better restaurants nearby have no wait. Diners assume 'If others are waiting, it must be good,' creating a self-reinforcing cycle where popularity itself becomes the product's main selling point.",
        "category": "personal"
      }
    ]
  },
  {
    "id": "groupthink",
    "title": "Groupthink",
    "category": "social",
    "summary": "In cohesive groups, the desire for harmony and conformity leads to irrational or dysfunctional decision-making. Dissent is suppressed, and critical evaluation is abandoned.",
    "why": "Groups develop strong norms and identity. Members fear rejection or conflict, so they self-censor doubts. Leaders may inadvertently signal preferred outcomes, and the illusion of unanimity makes dissent seem futile or disloyal.",
    "counter": "Assign a 'devil's advocate' role in group decisions. Encourage anonymous feedback. Leaders should withhold their opinions until others speak. Break into smaller subgroups to generate independent ideas before reconvening.",
    "source": "core",
    "tips": [
      "Assign a 'devil's advocate' role in group decisions",
      "Encourage anonymous feedback and criticism",
      "Leaders: withhold opinions until others have spoken",
      "Break into smaller subgroups before making final decisions"
    ],
    "examples": [
      {
        "title": "NASA Challenger Disaster",
        "description": "In 1986, NASA launched the Challenger despite engineers warning that O-rings would fail in cold weather. The night before launch, engineers presented data showing risk, but managers dismissed concerns to maintain the launch schedule. No one wanted to be 'the person who delayed the shuttle.' Seven astronauts died when the O-rings failed exactly as predicted.",
        "source": "NASA Challenger investigation",
        "year": 1986,
        "category": "historical"
      },
      {
        "title": "Bay of Pigs Invasion",
        "description": "President Kennedy's advisory group unanimously approved the CIA's plan to invade Cuba in 1961, despite multiple warning signs. Later, participants revealed they all had private doubts but assumed others' silence meant agreement. The invasion failed catastrophically within 3 days. Kennedy later said, 'How could I have been so stupid?'",
        "source": "Bay of Pigs invasion",
        "year": 1961,
        "category": "historical"
      },
      {
        "title": "WeWork's 2019 Valuation Collapse",
        "description": "WeWork's board and investors maintained a $47 billion valuation despite clear evidence of unsustainable losses and questionable governance. Insiders later admitted they had doubts but feared being labeled 'not a team player' or 'not understanding the vision.' Within months, the valuation collapsed to under $8 billion.",
        "source": "WeWork IPO failure",
        "year": 2019,
        "category": "business"
      }
    ]
  },
  {
    "id": "halo-effect",
    "title": "Halo Effect",
    "category": "perception",
    "summary": "One positive trait (attractiveness, success, likability) causes us to assume other positive traits. If someone is good-looking, we unconsciously assume they're also smart, kind, and competent.",
    "why": "Our brains seek cognitive efficiency by creating coherent narratives. If one trait is positive, assuming others are too creates a simpler, more consistent mental model than tracking each trait independently.",
    "counter": "Evaluate traits independently. Use structured assessment criteria. When impressed by one quality, deliberately ask: 'What evidence do I have for my other assumptions?' Separate appearance from competence, likability from trustworthiness.",
    "source": "core",
    "tips": [
      "Evaluate different traits independently, not as a package",
      "Use structured evaluation criteria for important judgments",
      "Ask: 'What evidence do I have for this specific quality?'",
      "Be aware that attractiveness influences your perception"
    ],
    "examples": [
      {
        "title": "Elizabeth Holmes and Theranos",
        "description": "Elizabeth Holmes raised $700 million for Theranos based largely on her charisma, Stanford pedigree, and Steve Jobs-like image. Investors overlooked red flags and lack of technical validation because they were impressed by her confidence and vision. When the technology was finally scrutinized, it was revealed to be fraudulent. The 'halo' of her persona prevented critical evaluation.",
        "source": "Theranos fraud case",
        "year": 2018,
        "category": "business"
      },
      {
        "title": "Attractive Defendants Get Lighter Sentences",
        "description": "A meta-analysis of 100+ studies found that physically attractive defendants receive significantly lighter sentences than unattractive defendants for identical crimes. Jurors unconsciously assume attractive people are less likely to be guilty and more likely to be good people, despite no correlation between appearance and criminal behavior.",
        "category": "historical"
      },
      {
        "title": "Apple Product Premium",
        "description": "Studies show consumers rate Apple products as higher quality and more innovative than identical or superior competing products, largely due to Apple's brand halo. In blind tests, people often can't distinguish Apple products from competitors, but when branded, Apple receives significantly higher ratings.",
        "category": "business"
      }
    ],
    "attribution": {
      "termCoinedBy": {
        "name": "Edward L. Thorndike",
        "year": 1920
      },
      "keyContributors": [
        "Edward L. Thorndike (1920) - Original observation in military ratings",
        "Solomon Asch (1946) - Extended research on impression formation"
      ],
      "researchConfidence": "high",
      "notes": "Thorndike first documented the halo effect in 1920 while studying military officer ratings. Asch later extended this work in his research on impression formation."
    }
  },
  {
    "id": "moral-luck",
    "title": "Moral Luck",
    "category": "social",
    "summary": "We judge actions by their outcomes rather than intentions. A drunk driver who hits someone is judged more harshly than one who makes it home safely, despite identical choices.",
    "why": "Outcomes are vivid and concrete, while intentions are abstract and uncertain. Our emotional responses are triggered by results, not hypotheticals. This bias may have evolutionary value in deterring harmful behaviors.",
    "counter": "Focus on the decision-making process and intentions, not just results. Ask: 'Was this a good decision given what they knew at the time?' Judge actions by their expected value, not their actual outcome. Separate luck from judgment.",
    "source": "core",
    "tips": [
      "Judge actions by intentions and available information, not outcomes",
      "Ask: 'Would I judge this differently if the outcome was different?'",
      "Consider counterfactuals: what could have happened but didn't",
      "Focus on decision quality, not just results"
    ]
  },
  {
    "id": "false-consensus",
    "title": "False Consensus",
    "category": "social",
    "summary": "We overestimate how many people share our beliefs, values, and behaviors. We assume our views are common sense and that those who disagree are unusual or biased.",
    "why": "We're surrounded by people similar to us (friends, family, colleagues) and consume media that confirms our views. This creates an echo chamber where our perspective seems universal. Our brains also project our own thoughts onto others.",
    "counter": "Actively seek out diverse viewpoints and demographics. Before assuming agreement, ask: 'What percentage of people actually think this way?' Use polls and data rather than intuition. Spend time with people unlike you.",
    "source": "core",
    "tips": [
      "Actively survey others before assuming agreement",
      "Seek out data on actual population beliefs and behaviors",
      "Challenge your assumption: 'Do I really know what others think?'",
      "Remember that your bubble isn't representative"
    ]
  },
  {
    "id": "curse-of-knowledge",
    "title": "Curse of Knowledge",
    "category": "perception",
    "summary": "Once we know something, we can't imagine not knowing it. Experts struggle to teach beginners because they forget what it's like to be ignorant of their field.",
    "why": "Knowledge fundamentally changes how we perceive information. Our brains can't 'unlearn' or easily simulate ignorance. What seems obvious now was once mysterious, but we lose access to that earlier mental state.",
    "counter": "When explaining, start from first principles. Ask: 'What did I find confusing when I first learned this?' Test your explanations on actual beginners. Use analogies to familiar concepts. Avoid jargon and define terms.",
    "source": "core",
    "tips": [
      "Explain concepts to someone unfamiliar to test your clarity",
      "Ask others what they understand, not just if they understand",
      "Use the 'five-year-old test': can you explain it simply?",
      "Remember what it was like before you learned this"
    ]
  },
  {
    "id": "spotlight-effect",
    "title": "Spotlight Effect",
    "category": "social",
    "summary": "We overestimate how much others notice our appearance, behavior, and mistakes. We feel like we're under a spotlight, but others are mostly focused on themselves.",
    "why": "We're the center of our own experience, so our actions and appearance feel highly salient. We lack access to others' internal focus, which is usually on their own concerns, not us.",
    "counter": "Remember that others are experiencing their own spotlight effect. Ask: 'Will anyone remember this in a week?' Most people are too worried about themselves to scrutinize you. Take social risks—the downside is smaller than you think.",
    "source": "core",
    "tips": [
      "Remind yourself: people are too busy thinking about themselves",
      "Ask friends what they noticed about you—usually it's less than you think",
      "Focus outward on others rather than inward on yourself",
      "Test your assumptions: did anyone actually notice that mistake?"
    ]
  },
  {
    "id": "availability-heuristic",
    "title": "Availability Heuristic",
    "category": "decision",
    "summary": "We judge probability by how easily examples come to mind. Plane crashes feel more likely than car accidents because they're more memorable and widely reported.",
    "why": "Ease of recall is usually a decent proxy for frequency—common things are easier to remember. But vivid, emotional, or recent events are disproportionately memorable, distorting our probability estimates. This heuristic works well in many situations but fails when media coverage, recency, or emotional impact makes rare events more mentally available than common ones.",
    "counter": "Look up actual statistics before judging risk. Ask: 'Am I remembering this because it's common or because it's dramatic?' Consider base rates. Be especially skeptical of fears triggered by recent news or personal anecdotes. Use systematic data rather than anecdotal evidence.",
    "source": "core",
    "researchLevel": "established",
    "tips": [
      "Seek out base rates and statistics, not just memorable examples",
      "Ask: 'Is this easy to recall because it's common or just vivid?'",
      "Counter dramatic news with boring statistical reality",
      "Keep a log of actual frequencies vs. your gut feelings"
    ],
    "examples": [
      {
        "title": "Post-9/11 Driving Deaths",
        "description": "After 9/11, many Americans avoided flying due to terrorism fears and drove instead. This led to an estimated 1,600 additional traffic deaths in the year following the attacks—more than died on the planes. The vivid, emotional memory of 9/11 made flying feel more dangerous than driving, despite driving being statistically far more risky.",
        "source": "Traffic safety research",
        "year": 2002,
        "category": "historical"
      },
      {
        "title": "Shark Attack Summer of 2001",
        "description": "In summer 2001, media extensively covered shark attacks, leading to widespread beach fears and tourism drops. People believed shark attacks were increasing dramatically. In reality, 2001 had fewer shark attacks than average—but the coverage made them more 'available' in memory, distorting risk perception.",
        "source": "Media coverage analysis",
        "year": 2001,
        "category": "news"
      },
      {
        "title": "Lottery Ticket Sales After Big Wins",
        "description": "Lottery ticket sales spike dramatically after someone wins a large jackpot, especially in the same region. The vivid story of a winner makes winning feel more probable, even though the odds remain astronomically low (often 1 in 300 million). The availability of the winner's story overrides statistical reality.",
        "category": "personal"
      }
    ],
    "references": [
      {
        "title": "Judgment Under Uncertainty: Heuristics and Biases",
        "authors": "Tversky, A., & Kahneman, D.",
        "year": 1974,
        "journal": "Science",
        "type": "study"
      },
      {
        "title": "Availability: A Heuristic for Judging Frequency and Probability",
        "authors": "Tversky, A., & Kahneman, D.",
        "year": 1973,
        "journal": "Cognitive Psychology",
        "type": "study"
      }
    ],
    "attribution": {
      "termCoinedBy": {
        "name": "Amos Tversky & Daniel Kahneman",
        "year": 1973
      },
      "keyContributors": [
        "Amos Tversky & Daniel Kahneman (1973, 1974) - Introduced and systematized",
        "Multiple replications and extensions since 1970s"
      ],
      "researchConfidence": "high",
      "notes": "Introduced by Tversky & Kahneman as part of their heuristics and biases program. One of the most well-replicated findings in cognitive psychology."
    }
  },
  {
    "id": "defensive-attribution",
    "title": "Defensive Attribution",
    "category": "social",
    "summary": "We blame victims for their misfortune to maintain the belief that bad things won't happen to us. If they made a mistake, we can avoid it; if it was random, we're vulnerable too.",
    "why": "Believing the world is just and controllable reduces anxiety. If victims are responsible for their fate, we can protect ourselves by being smarter or more careful. Randomness is psychologically threatening.",
    "counter": "Recognize that bad things happen to good people through no fault of their own. Practice empathy by imagining yourself in their situation. Acknowledge the role of luck and systemic factors. Support victims instead of judging them.",
    "source": "core",
    "tips": [
      "Acknowledge that bad things can happen to anyone, including you, regardless of precautions",
      "Show compassion for victims without searching for ways they 'caused' their misfortune",
      "Ask yourself: 'Am I blaming them to feel safer or more in control?'",
      "Recognize when you're victim-blaming to protect your sense of security"
    ]
  },
  {
    "id": "just-world-hypothesis",
    "title": "Just-World Hypothesis",
    "category": "social",
    "summary": "We believe the world is fundamentally fair: good things happen to good people, bad things to bad people. This leads us to blame victims and assume success indicates virtue.",
    "why": "A just world is psychologically comforting and gives us a sense of control. If outcomes match moral worth, we can secure good outcomes by being good. Accepting randomness and injustice is anxiety-inducing.",
    "counter": "Acknowledge that life is often unfair and random. Success often involves luck, privilege, and timing. Suffering doesn't imply wrongdoing. Focus on systemic factors and circumstances rather than assuming moral causation.",
    "source": "core",
    "tips": [
      "Recognize that life isn't fair—outcomes don't always match merit",
      "Support systemic solutions, not just individual responsibility",
      "Show compassion without needing victims to 'deserve' it",
      "Ask: 'Am I assuming fairness to feel better about inequality?'"
    ]
  },
  {
    "id": "naive-realism",
    "title": "Naive Realism",
    "category": "perception",
    "summary": "We believe we see reality objectively and that those who disagree are uninformed, irrational, or biased. We don't recognize our own biases and interpretive frameworks.",
    "why": "Our perceptions feel direct and unmediated—we don't experience the brain's interpretive processes. This creates the illusion that we're seeing 'the truth' while others are distorting it through their biases.",
    "counter": "Assume you have blind spots and biases you can't see. Ask: 'What would make a reasonable person disagree with me?' Seek out steel-man arguments. Recognize that your perspective is one interpretation, not objective reality.",
    "source": "core",
    "tips": [
      "Remind yourself: 'My perception is an interpretation, not reality'",
      "Actively seek out how others see the same situation differently",
      "Ask: 'What might I be missing due to my perspective?'",
      "Practice intellectual humility: you could be wrong"
    ]
  },
  {
    "id": "naive-cynicism",
    "title": "Naive Cynicism",
    "category": "social",
    "summary": "We believe others are more biased and self-interested than they actually are, while seeing ourselves as objective. We assume hidden selfish motives behind others' actions.",
    "why": "We have access to our own good intentions but only see others' actions. Cynicism feels sophisticated and protects us from being exploited. Assuming self-interest is often a safe bet, but we overapply it.",
    "counter": "Give people the benefit of the doubt. Ask: 'What innocent explanation could there be?' Remember that most people see themselves as good and well-intentioned. Test your cynical assumptions against evidence.",
    "source": "core",
    "tips": [
      "Ask: 'What evidence do I have for their bad motives?'",
      "Consider benign explanations before assuming malice",
      "Test your cynicism by asking people directly about their motives",
      "Remember: not everyone who disagrees is corrupt or biased"
    ]
  },
  {
    "id": "forer-barnum-effect",
    "title": "Forer/Barnum Effect",
    "category": "perception",
    "summary": "We believe vague, general personality descriptions are uniquely accurate for us. This explains why horoscopes, fortune-telling, and personality tests feel so personal.",
    "why": "We seek self-knowledge and confirmation of our uniqueness. Vague statements can apply to almost anyone, but we selectively focus on the parts that fit and interpret ambiguity in self-flattering ways.",
    "counter": "Be skeptical of personality assessments that feel 'eerily accurate.' Ask: 'Could this apply to most people?' Look for specific, falsifiable claims. Test whether the same description resonates with others.",
    "source": "core",
    "tips": [
      "Ask: 'Is this specific to me, or could it apply to anyone?'",
      "Seek falsifiable, specific claims rather than vague generalities",
      "Test personality assessments with friends—do they fit too?",
      "Be skeptical of cold reading techniques and vague predictions"
    ]
  },
  {
    "id": "dunning-kruger-effect",
    "title": "Dunning–Kruger Effect",
    "category": "perception",
    "summary": "Incompetent people overestimate their ability because they lack the knowledge to recognize their incompetence. Experts underestimate their relative ability, assuming tasks easy for them are easy for everyone.",
    "why": "Skill and self-assessment ability are linked—you need knowledge to recognize what you don't know. Beginners don't know what they don't know. Experts forget that their skills are rare.",
    "counter": "Seek feedback from knowledgeable others. Before claiming expertise, ask: 'What would an actual expert know that I don't?' Study the field enough to understand its depth. Embrace intellectual humility.",
    "source": "core",
    "tips": [
      "Assume you know less than you think, especially when starting",
      "Seek honest feedback from experts in the field",
      "Track your confidence vs. actual performance over time",
      "Embrace 'learned ignorance': the more you know, the more you see what you don't"
    ],
    "examples": [
      {
        "title": "Cryptocurrency 'Experts' in 2021",
        "description": "During the 2021 crypto boom, thousands of people with minimal financial knowledge confidently gave investment advice on social media, predicting specific price targets and claiming expertise. Many had only been involved for months but spoke with certainty. When the market crashed in 2022, most disappeared. Their lack of knowledge prevented them from recognizing how much they didn't know about markets, regulation, and technology.",
        "source": "Cryptocurrency market analysis",
        "year": 2021,
        "category": "business"
      },
      {
        "title": "American Idol Auditions",
        "description": "The show's producers noted that the worst singers were often the most confident, genuinely shocked when judges criticized them. They lacked the musical knowledge to assess their own performance. Meanwhile, talented singers often expressed doubt and nervousness because they understood the complexity of singing well.",
        "category": "personal"
      },
      {
        "title": "COVID-19 Armchair Epidemiologists",
        "description": "In 2020, millions of people with no epidemiology training confidently disputed expert recommendations, claiming 'I did my own research.' Studies showed inverse correlation between actual knowledge and confidence in contradicting experts. Those with minimal understanding were most certain they knew better than epidemiologists who had studied infectious diseases for decades.",
        "source": "COVID-19 misinformation studies",
        "year": 2020,
        "category": "news"
      }
    ],
    "attribution": {
      "termCoinedBy": {
        "name": "David Dunning & Justin Kruger",
        "year": 1999
      },
      "keyContributors": [
        "David Dunning & Justin Kruger (1999) - Original research",
        "Subsequent research has refined understanding of the effect"
      ],
      "researchConfidence": "high",
      "notes": "Dunning & Kruger's 1999 paper introduced this specific formulation. The general phenomenon of metacognitive calibration had been noted earlier, but their work systematized it."
    }
  },
  {
    "id": "anchoring",
    "title": "Anchoring",
    "category": "decision",
    "summary": "We rely too heavily on the first piece of information we receive. Initial numbers, even if random or irrelevant, disproportionately influence our estimates and decisions.",
    "why": "Our brains use anchors as starting points and adjust from there, but we typically don't adjust enough. The anchor sets a reference point that constrains our thinking, even when we know it's arbitrary. This bias occurs because anchoring provides a cognitive shortcut that reduces mental effort, but the adjustment process is often insufficient.",
    "counter": "Generate your own estimate before seeing others' numbers. Ask: 'What would I think if the anchor were different?' Consider the range of possibilities independently. Be especially wary of anchors in negotiations. Use multiple reference points to avoid single-anchor bias.",
    "source": "core",
    "researchLevel": "established",
    "tips": [
      "Ignore the first number you see—it's often arbitrary or strategic",
      "Generate your own estimate before seeing others' numbers",
      "Ask: 'What evidence supports this number beyond the anchor?'",
      "In negotiations, be the first to anchor with an aggressive number"
    ],
    "examples": [
      {
        "title": "Real Estate Listing Prices",
        "description": "Studies show that initial listing prices strongly influence final sale prices, even when buyers know the listing is arbitrary. A house listed at $500,000 sells for significantly more than the same house listed at $450,000, even after negotiations. Buyers anchor on the listing price and adjust insufficiently, even when they consciously try to ignore it.",
        "category": "business"
      },
      {
        "title": "Salary Negotiation Research",
        "description": "Research shows that whoever makes the first offer in salary negotiations sets an anchor that predicts the final outcome. When employers open with $60,000, final salaries average $65,000. When candidates open with $80,000 for the same role, final salaries average $75,000. The first number disproportionately influences the entire negotiation.",
        "category": "business"
      },
      {
        "title": "Restaurant Menu Pricing",
        "description": "Restaurants strategically place an expensive item at the top of the menu (e.g., $95 steak) to make other items seem reasonable by comparison. Studies show diners spend 15-20% more when menus start with high-priced anchors, even though most people don't order the anchor item. The expensive option reframes what 'normal' pricing means.",
        "category": "personal"
      }
    ],
    "references": [
      {
        "title": "Judgment Under Uncertainty: Heuristics and Biases",
        "authors": "Tversky, A., & Kahneman, D.",
        "year": 1974,
        "journal": "Science",
        "type": "study"
      },
      {
        "title": "The Anchoring-and-Adjustment Heuristic",
        "authors": "Epley, N., & Gilovich, T.",
        "year": 2001,
        "journal": "Journal of Personality and Social Psychology",
        "type": "study"
      }
    ],
    "attribution": {
      "termCoinedBy": {
        "name": "Amos Tversky & Daniel Kahneman",
        "year": 1974
      },
      "keyContributors": [
        "Amos Tversky & Daniel Kahneman (1974) - Original demonstration",
        "Nicholas Epley & Thomas Gilovich (2001) - Extended research on adjustment"
      ],
      "researchConfidence": "high",
      "notes": "Introduced by Tversky & Kahneman in their 1974 Science paper. Extensively replicated across many domains including judgment, negotiation, and decision-making."
    }
  },
  {
    "id": "automation-bias",
    "title": "Automation Bias",
    "category": "decision",
    "summary": "We over-trust automated systems and algorithms, even when they're wrong. We defer to computer recommendations and fail to question or override them when we should.",
    "why": "Technology feels objective and infallible. Questioning it requires effort and confidence. We're also trained to trust authority, and algorithms have authority. Overriding automation feels like taking responsibility for potential errors.",
    "counter": "Remember that algorithms are built by humans with biases and limitations. Maintain critical thinking and domain expertise. Ask: 'Does this recommendation make sense?' Be willing to override automation when it conflicts with your judgment.",
    "source": "core",
    "tips": [
      "Question algorithmic recommendations, especially in critical decisions",
      "Maintain expertise so you can override automation when needed",
      "Ask: 'What could the algorithm miss or get wrong?'",
      "Use automation as input, not as decision-maker"
    ]
  },
  {
    "id": "google-effect",
    "title": "Google Effect (Digital Amnesia)",
    "category": "memory",
    "summary": "We forget information that's easily accessible online. We remember where to find information rather than the information itself, outsourcing memory to search engines.",
    "why": "Our brains optimize for efficiency. If information is reliably available externally, there's less need to store it internally. This is a form of transactive memory—using external systems as memory extensions.",
    "counter": "For important information, practice active recall and spaced repetition. Take notes by hand. Engage deeply with material rather than skimming. Recognize that understanding requires internalization, not just access.",
    "source": "core",
    "tips": [
      "Practice active recall before Googling answers",
      "Take notes by hand to deepen memory encoding",
      "Ask yourself: 'Can I explain this without looking it up?'",
      "Build genuine understanding, not just bookmark navigation skills"
    ]
  },
  {
    "id": "reactance",
    "title": "Reactance",
    "category": "social",
    "summary": "When our freedom is threatened, we react by wanting the forbidden thing even more. Tell someone they can't do something, and they'll want to do it just to assert autonomy.",
    "why": "We value autonomy and resist control. Restrictions trigger a defensive response to preserve our sense of freedom. This is especially strong when the restriction feels arbitrary or when we feel our identity is threatened.",
    "counter": "When you feel reactance, pause and ask: 'Do I actually want this, or am I just resisting being told what to do?' Separate your authentic preferences from your defensive reactions. When persuading others, emphasize choice and autonomy.",
    "source": "core",
    "tips": [
      "Notice when you resist simply because you feel pressured",
      "Reframe restrictions as choices: 'I'm choosing to...'",
      "Ask: 'Would I want this if it wasn't forbidden or mandated?'",
      "Give others autonomy instead of direct orders when possible"
    ]
  },
  {
    "id": "confirmation-bias",
    "title": "Confirmation Bias",
    "category": "decision",
    "summary": "We seek, interpret, and remember information that confirms our existing beliefs while ignoring or dismissing contradictory evidence. We're not truth-seekers; we're confirmation-seekers.",
    "why": "Confirming beliefs feels good and is cognitively easy. Challenging them is uncomfortable and requires effort. Our brains are motivated reasoners—we use logic to defend conclusions we've already reached emotionally. This bias is particularly strong when beliefs are tied to our identity or when we have invested significant time or resources in a particular viewpoint.",
    "counter": "Actively seek disconfirming evidence. Ask: 'What would prove me wrong?' Engage with the strongest counterarguments (steel-man, not straw-man). Keep a list of beliefs you've changed to normalize updating your views. Practice steelmanning opposing arguments.",
    "source": "core",
    "researchLevel": "established",
    "tips": [
      "Actively seek out information that challenges your beliefs",
      "Ask: 'What would prove me wrong?' then look for that evidence",
      "Follow people with different viewpoints on social media",
      "Play devil's advocate with your own opinions"
    ],
    "examples": [
      {
        "title": "Iraq WMD Intelligence Failure",
        "description": "U.S. intelligence agencies were convinced Iraq had weapons of mass destruction before the 2003 invasion. Analysts selectively focused on evidence supporting WMDs while dismissing contradictory intelligence. Post-war investigations found that disconfirming evidence was systematically ignored or reinterpreted to fit the WMD narrative. No WMDs were ever found.",
        "source": "Senate Intelligence Committee Report",
        "year": 2004,
        "category": "politics"
      },
      {
        "title": "Medical Misdiagnosis Patterns",
        "description": "Studies show doctors who form an initial diagnosis often seek confirming evidence while ignoring contradictory symptoms. In one study, doctors shown the same patient data reached different diagnoses based on initial hunches, then interpreted identical test results as supporting their respective theories. This contributes to an estimated 10-15% misdiagnosis rate.",
        "category": "business"
      },
      {
        "title": "Social Media Echo Chambers",
        "description": "A 2023 study found that 87% of political content people engage with on social media confirms their existing views. Algorithms amplify this by showing more confirming content. Users actively avoid, unfriend, or dismiss sources that challenge their beliefs, creating self-reinforcing information bubbles that make belief change nearly impossible.",
        "source": "Social media research",
        "year": 2023,
        "category": "news"
      }
    ],
    "references": [
      {
        "title": "Confirmation Bias: A Ubiquitous Phenomenon in Many Guises",
        "authors": "Nickerson, R. S.",
        "year": 1998,
        "journal": "Review of General Psychology",
        "type": "review"
      },
      {
        "title": "The Psychology of Confirmation Bias",
        "authors": "Klayman, J.",
        "year": 1995,
        "journal": "Psychological Bulletin",
        "type": "study"
      }
    ],
    "attribution": {
      "termCoinedBy": null,
      "keyContributors": [
        "Peter C. Wason (1960s) - Early experimental demonstrations",
        "Raymond S. Nickerson (1998) - Comprehensive review",
        "Joshua Klayman (1995) - Theoretical refinement"
      ],
      "researchConfidence": "high",
      "notes": "Wason's work in the 1960s provided early experimental demonstrations. The term and concept have been developed by many researchers. Nickerson's 1998 review is widely cited."
    }
  },
  {
    "id": "backfire-effect",
    "title": "Backfire Effect",
    "category": "social",
    "summary": "When our core beliefs are challenged with evidence, we sometimes believe them even more strongly. Corrections can paradoxically reinforce the misconceptions they're meant to fix.",
    "why": "Core beliefs are tied to identity and worldview. Threats to them trigger defensive reasoning. We counterargue the evidence, and the act of generating defenses strengthens our original belief. Admitting error feels like losing status.",
    "counter": "Separate beliefs from identity. Practice saying 'I was wrong' about small things to normalize it. When presenting corrections, affirm the person's values first. Focus on shared goals rather than proving someone wrong.",
    "source": "core",
    "tips": [
      "When corrected, pause and breathe before defending yourself",
      "Separate your identity from your beliefs—you can change your mind",
      "Ask: 'What would I need to see to change my view?'",
      "Practice saying: 'I didn't know that, thank you'"
    ]
  },
  {
    "id": "third-person-effect",
    "title": "Third-Person Effect",
    "category": "perception",
    "summary": "We believe others are more influenced by media, advertising, and propaganda than we are. We see ourselves as uniquely resistant to manipulation while others are susceptible.",
    "why": "We have introspective access to our own reasoning and feel in control of our thoughts. We don't see the subtle influences on our cognition. Believing we're less influenced than others protects our self-image as rational and independent.",
    "counter": "Assume you're as susceptible to influence as anyone else. Ask: 'How might this be affecting me without my awareness?' Study how persuasion and manipulation work. Be humble about your cognitive vulnerabilities.",
    "source": "core",
    "tips": [
      "Assume you're as susceptible to influence as others",
      "Ask: 'How has advertising/media affected my choices?'",
      "Track your purchases and beliefs to spot external influences",
      "Remember: thinking you're immune often makes you more vulnerable"
    ]
  },
  {
    "id": "belief-bias",
    "title": "Belief Bias",
    "category": "decision",
    "summary": "We judge arguments by whether we agree with the conclusion rather than the quality of the logic. Valid reasoning that leads to disagreeable conclusions feels wrong; flawed reasoning that confirms our beliefs feels right.",
    "why": "Evaluating logic is hard; checking conclusions against our beliefs is easy. Our brains take shortcuts. We're motivated to accept arguments that support our views and reject those that threaten them, regardless of logical validity.",
    "counter": "Evaluate arguments independently of whether you like the conclusion. Ask: 'Is this reasoning sound, even if I disagree?' Practice formal logic. Try to find flaws in arguments you agree with and strengths in those you oppose.",
    "source": "core",
    "tips": [
      "Evaluate arguments by their logical structure, not conclusions",
      "Practice analyzing arguments you agree with for flaws",
      "Ask: 'Is this logically valid, or do I just like the conclusion?'",
      "Study formal logic to separate validity from believability"
    ]
  },
  {
    "id": "availability-cascade",
    "title": "Availability Cascade",
    "category": "social",
    "summary": "A self-reinforcing cycle where a belief gains plausibility through repetition in public discourse. The more we hear something, the more true it seems, leading to more repetition.",
    "why": "Repeated exposure increases familiarity, which our brains mistake for truth. Media coverage creates availability, making the issue feel important and common. This triggers more coverage, creating a feedback loop.",
    "counter": "Distinguish between how often you hear something and how true or important it is. Ask: 'Is this actually common, or just commonly discussed?' Look for base rates and statistics. Be skeptical of media-driven panics.",
    "source": "core",
    "tips": [
      "Question claims that 'everyone is talking about' something",
      "Seek primary sources and original data, not just repeated stories",
      "Ask: 'Is this widespread because it's true or just repeated?'",
      "Wait for dust to settle before forming opinions on viral topics"
    ]
  },
  {
    "id": "declinism",
    "title": "Declinism",
    "category": "perception",
    "summary": "We believe things are getting worse, that the past was better, and that the future is bleak. Every generation thinks the next one is ruining everything.",
    "why": "We remember the past selectively, filtering out the bad and romanticizing the good. Current problems are vivid and salient. Negative news is more memorable and widely reported. Aging changes our relationship to culture and society.",
    "counter": "Study history and data—most metrics show improvement over time. Ask: 'What evidence do I have beyond my feelings?' Remember that every generation has felt this way. Distinguish between personal nostalgia and objective decline.",
    "source": "core",
    "tips": [
      "Seek out historical data to counter nostalgia and pessimism",
      "Read books like 'Factfulness' or 'Enlightenment Now' for perspective",
      "Ask: 'What has actually gotten better over time?'",
      "Remember: nostalgia edits out the bad parts of the past"
    ]
  },
  {
    "id": "status-quo-bias",
    "title": "Status Quo Bias",
    "category": "decision",
    "summary": "We prefer things to stay the same. Change feels risky and effortful, so we stick with current situations even when better alternatives exist. Inertia is powerful.",
    "why": "Change requires energy and involves uncertainty. The current situation is known and feels safe. We also feel greater regret for bad outcomes resulting from action than from inaction, making change feel riskier.",
    "counter": "Regularly question defaults. Ask: 'If I were starting fresh, would I choose this?' Imagine the status quo as a new option you're actively choosing. Consider the costs of inaction, not just action. Run experiments.",
    "source": "core",
    "tips": [
      "Regularly reassess defaults and current arrangements",
      "Ask: 'Would I choose this if starting from scratch today?'",
      "Experiment with small changes to test alternatives",
      "Remember: the current state isn't necessarily optimal"
    ]
  },
  {
    "id": "sunk-cost-fallacy",
    "title": "Sunk Cost Fallacy",
    "category": "decision",
    "summary": "We continue investing in something because we've already invested, even when it's clearly not working. We throw good money after bad to avoid 'wasting' what we've already spent.",
    "why": "Admitting a loss feels like failure. We want to believe our past investments were worthwhile. Quitting feels like waste, even though sunk costs are already gone and shouldn't influence future decisions.",
    "counter": "Ignore sunk costs—they're gone regardless of what you do next. Ask: 'If I were starting today with no prior investment, would I choose this?' Focus on future costs and benefits only. Practice cutting losses early.",
    "source": "core",
    "tips": [
      "Focus on future costs and benefits, not past investments",
      "Ask: 'If I hadn't already invested, would I start now?'",
      "Practice cutting losses early in low-stakes situations",
      "Remember: you can't recover sunk costs by doubling down"
    ],
    "examples": [
      {
        "title": "Concorde Supersonic Jet",
        "description": "The British and French governments continued funding the Concorde project for decades despite clear evidence it would never be profitable. They had already invested billions and couldn't admit the project was a failure. The term 'Concorde Fallacy' became synonymous with sunk cost thinking. The plane operated at a loss for its entire commercial life.",
        "source": "Concorde project history",
        "year": 1976,
        "category": "historical"
      },
      {
        "title": "Movie Walkouts and Bad Relationships",
        "description": "Studies show people are more likely to sit through a terrible movie if they paid full price versus getting a free ticket, even though the money is gone either way. Similarly, people stay in unfulfilling relationships longer when they've invested more time, even when future happiness would be greater by leaving. The past investment shouldn't matter, but it does.",
        "category": "personal"
      },
      {
        "title": "Blockbuster vs. Netflix",
        "description": "Blockbuster invested billions in physical stores and late fees revenue. When Netflix offered to sell for $50 million in 2000, Blockbuster refused because it would mean admitting their store model was obsolete. They continued investing in stores even as streaming became dominant. Blockbuster declared bankruptcy in 2010; Netflix is worth over $150 billion today.",
        "source": "Business case study",
        "year": 2010,
        "category": "business"
      }
    ],
    "attribution": {
      "termCoinedBy": null,
      "keyContributors": [
        "Hal R. Arkes & Catherine Blumer (1985) - Systematic research",
        "Richard Thaler (1980) - Economic perspective",
        "Earlier observations in economics and psychology"
      ],
      "researchConfidence": "high",
      "notes": "Arkes & Blumer's 1985 paper provided systematic experimental demonstration. The concept has roots in economics (sunk costs) and was observed in various contexts earlier."
    }
  },
  {
    "id": "gamblers-fallacy",
    "title": "Gambler's Fallacy",
    "category": "decision",
    "summary": "We believe that past random events influence future ones. After several coin flips landing heads, we think tails is 'due,' even though each flip is independent with 50/50 odds.",
    "why": "We see patterns in randomness and expect small samples to reflect overall probabilities. Our brains evolved to detect patterns, so we see causation and balance where there's only chance.",
    "counter": "Remember that random events have no memory. Each independent trial has the same odds regardless of history. Study probability and statistics. Be especially careful with this in gambling and investing.",
    "source": "core",
    "tips": [
      "Remember: past random events don't influence future ones",
      "Ask: 'Does this process have memory?' (Usually no)",
      "Study probability to understand independence of events",
      "In truly random processes, 'due for a win' doesn't exist"
    ]
  },
  {
    "id": "zero-risk-bias",
    "title": "Zero-Risk Bias",
    "category": "decision",
    "summary": "We prefer eliminating small risks entirely over significantly reducing larger risks. We'll pay more to reduce risk from 1% to 0% than from 10% to 5%, even though the latter saves more lives.",
    "why": "Zero feels qualitatively different from small probabilities. Complete elimination provides psychological closure and certainty. We're also bad at intuiting small probability differences—1% vs 0% feels bigger than 10% vs 5%.",
    "counter": "Focus on absolute risk reduction, not relative. Ask: 'Which option saves more lives or resources?' Be willing to accept small residual risks if it allows greater risk reduction elsewhere. Think in terms of expected value.",
    "source": "core",
    "tips": [
      "Compare actual risk reduction, not just 'zero vs. some risk'",
      "Ask: 'What's the cost-benefit of eliminating this small risk?'",
      "Focus resources on reducing large risks, not eliminating small ones",
      "Remember: you can't eliminate all risk, so prioritize wisely"
    ]
  },
  {
    "id": "framing-effect",
    "title": "Framing Effect",
    "category": "decision",
    "summary": "We react differently to the same information depending on how it's presented. '90% survival rate' sounds better than '10% mortality rate,' even though they're identical.",
    "why": "Our brains respond to the emotional tone and reference point of information, not just its logical content. Positive frames trigger approach; negative frames trigger avoidance. We're not purely rational calculators.",
    "counter": "Reframe information in multiple ways before deciding. Ask: 'How would I feel if this were presented differently?' Look for the underlying numbers and probabilities. Be aware of how others might be framing information to influence you.",
    "source": "core",
    "tips": [
      "Reframe information in multiple ways before deciding",
      "Ask: 'How would this sound if described differently?'",
      "Convert percentages to frequencies and vice versa",
      "Be aware of 'loss framing' vs. 'gain framing' in persuasion"
    ],
    "attribution": {
      "termCoinedBy": {
        "name": "Amos Tversky & Daniel Kahneman",
        "year": 1981
      },
      "keyContributors": [
        "Amos Tversky & Daniel Kahneman (1981) - Prospect theory and framing",
        "Extensive research on decision framing across domains"
      ],
      "researchConfidence": "high",
      "notes": "Systematically demonstrated in Tversky & Kahneman's prospect theory work. Framing effects are among the most robust findings in behavioral decision research."
    }
  },
  {
    "id": "stereotyping",
    "title": "Stereotyping",
    "category": "social",
    "summary": "We apply generalized beliefs about groups to individuals, ignoring their unique characteristics. We assume people will behave according to group stereotypes rather than their individual traits.",
    "why": "Stereotypes are cognitive shortcuts that reduce the complexity of social perception. They're often based on some statistical reality but are overgeneralized and applied rigidly, ignoring individual variation.",
    "counter": "Treat people as individuals, not representatives of groups. Ask: 'What evidence do I have about this specific person?' Expose yourself to counter-stereotypical examples. Recognize that within-group variation is usually larger than between-group differences.",
    "source": "core",
    "tips": [
      "Treat people as individuals, not representatives of groups",
      "Ask: 'What evidence do I have about this specific person?'",
      "Actively counter stereotypes with counter-examples",
      "Build genuine relationships with people from stereotyped groups"
    ]
  },
  {
    "id": "out-group-homogeneity-bias",
    "title": "Out-group Homogeneity Bias",
    "category": "social",
    "summary": "We see members of other groups as more similar to each other than they actually are, while recognizing diversity within our own group. 'They all look/think/act the same.'",
    "why": "We have more exposure to and attention for our own group, so we notice individual differences. We have less contact with out-groups and process them more categorically, focusing on group-defining features.",
    "counter": "Seek meaningful contact with out-group members. Ask: 'What individual differences am I missing?' Remember that every group has as much internal diversity as your own. Focus on individual characteristics, not group membership.",
    "source": "core",
    "tips": [
      "Remember: 'they' are just as diverse as 'we' are",
      "Seek out individual stories from out-group members",
      "Ask: 'Am I seeing real similarity or just unfamiliarity?'",
      "Spend meaningful time with out-group members"
    ]
  },
  {
    "id": "authority-bias",
    "title": "Authority Bias",
    "category": "social",
    "summary": "We over-trust and obey authority figures, even when they're wrong or acting unethically. Titles, credentials, and status make us less likely to question or resist.",
    "why": "Respecting authority is socially adaptive and was crucial for survival in hierarchical societies. Authorities often do have expertise. Challenging them risks conflict and social consequences. We're trained from childhood to defer to authority.",
    "counter": "Evaluate ideas on their merits, not the speaker's status. Ask: 'What's the actual evidence?' Remember that experts can be wrong, especially outside their domain. Be willing to respectfully question authority when something seems off.",
    "source": "core",
    "tips": [
      "Question expert claims, especially outside their expertise",
      "Ask: 'What evidence supports this, beyond their credentials?'",
      "Remember: experts can be wrong, especially on new issues",
      "Evaluate arguments on merit, not just the speaker's status"
    ]
  },
  {
    "id": "placebo-effect",
    "title": "Placebo Effect",
    "category": "perception",
    "summary": "We experience real improvements from fake treatments because we believe they'll work. Expectations and beliefs can trigger genuine physiological and psychological changes.",
    "why": "Mind and body are interconnected. Expectations activate neural pathways associated with relief and healing. Attention, ritual, and hope have real effects. Our brains can modulate pain, mood, and even immune function based on beliefs.",
    "counter": "Recognize the power of expectation and context in healing. Use it ethically to enhance legitimate treatments. In research and decision-making, use blinded controls to separate placebo effects from actual efficacy.",
    "source": "core",
    "tips": [
      "Be aware that your expectations can create real physiological changes",
      "Recognize that feeling better doesn't always mean the treatment caused it",
      "In research or testing, use control groups and blinding to account for placebo effects",
      "Remember: subjective improvement may be due to expectations, not the treatment itself"
    ]
  },
  {
    "id": "survivorship-bias",
    "title": "Survivorship Bias",
    "category": "decision",
    "summary": "We focus on successes that survived a selection process while ignoring failures that didn't. We study successful companies and conclude their strategies work, forgetting the many that used the same strategies and failed.",
    "why": "Survivors are visible; failures disappear. Success stories are inspiring and widely shared; failures are forgotten or hidden. This creates a distorted sample that makes success seem more predictable and replicable than it is.",
    "counter": "Actively seek out failures and near-misses. Ask: 'How many tried this and failed?' Study base rates of success. Be skeptical of advice from successful people—their strategies may not have caused their success. Consider selection effects.",
    "source": "core",
    "tips": [
      "Actively seek out stories of failure, not just success",
      "Ask: 'What about those who tried the same thing and failed?'",
      "Look at base rates, not just visible successes",
      "Remember: the graveyard of failed attempts is usually invisible"
    ]
  },
  {
    "id": "tachypsychia",
    "title": "Tachypsychia",
    "category": "perception",
    "summary": "Time seems to slow down during high-stress or dangerous situations. Moments of crisis feel like they unfold in slow motion, though our actual reaction time doesn't improve.",
    "why": "During stress, the amygdala becomes highly active and encodes memories more densely. When we recall the event, the richness of detail makes it seem like more time passed. Attention also narrows and intensifies, altering time perception.",
    "counter": "Recognize that time distortion is a perceptual effect, not reality. In crisis, you're not actually moving faster or thinking more clearly. Train and prepare for high-stress situations so you can respond effectively despite altered perception.",
    "source": "core",
    "tips": [
      "In high-stress moments, remember time perception is distorted",
      "Practice emergency scenarios to improve actual reaction time",
      "Don't rely on subjective time estimates during crises",
      "Use video replay to calibrate time perception after events"
    ]
  },
  {
    "id": "law-of-triviality",
    "title": "Law of Triviality (Bike-Shedding)",
    "category": "decision",
    "summary": "We spend disproportionate time on trivial decisions because they're easy to understand, while glossing over complex, important ones. Committees debate the color of a bike shed for hours but approve a nuclear reactor in minutes.",
    "why": "Complex topics require expertise and effort to evaluate, so we defer to experts or avoid engagement. Trivial topics are accessible to everyone, so everyone has opinions and wants to contribute. Participation feels good.",
    "counter": "Allocate time proportional to importance, not ease of understanding. Ask: 'What's the actual impact of this decision?' Defer to expertise on complex matters. Limit discussion time on trivial issues. Focus energy on high-leverage decisions.",
    "source": "core",
    "tips": [
      "Allocate meeting time based on importance, not accessibility",
      "Ask: 'Are we debating this because it's important or easy?'",
      "Set time limits on trivial discussions",
      "Focus on high-impact decisions even if they're complex"
    ]
  },
  {
    "id": "zeigarnik-effect",
    "title": "Zeigarnik Effect",
    "category": "memory",
    "summary": "We remember incomplete or interrupted tasks better than completed ones. Unfinished business stays in our minds, creating mental tension until we complete it.",
    "why": "Our brains maintain active representations of ongoing goals. Completion provides closure and allows the brain to release the task from active memory. Incompletion creates cognitive tension that keeps the task salient.",
    "counter": "Use this to your advantage: start tasks to create momentum and mental commitment. Break large projects into smaller tasks to get frequent completion satisfaction. When stuck, take breaks and let your unconscious work on it.",
    "source": "core",
    "tips": [
      "Use this positively: start tasks to create completion motivation",
      "Write down unfinished tasks before breaks to reduce mental clutter",
      "Break large projects into completable chunks for satisfaction",
      "Create closure rituals for work you must pause"
    ]
  },
  {
    "id": "ikea-effect",
    "title": "IKEA Effect",
    "category": "perception",
    "summary": "We value things more when we've put effort into creating them. Self-assembled furniture, home-cooked meals, and our own ideas seem better than they objectively are.",
    "why": "Effort creates a sense of ownership and investment. We want to believe our labor was worthwhile, so we inflate the value of the result. This also signals competence to ourselves and others.",
    "counter": "Seek objective feedback on your creations. Ask: 'Would I value this if someone else made it?' Be willing to abandon or improve your work based on external input. Separate effort from quality.",
    "source": "core",
    "tips": [
      "Get external feedback on things you've created or assembled",
      "Ask: 'Would I value this if someone else made it?'",
      "Be aware you're attached due to effort, not just quality",
      "When selling, emphasize DIY/customization to leverage this bias"
    ]
  },
  {
    "id": "ben-franklin-effect",
    "title": "Ben Franklin Effect",
    "category": "social",
    "summary": "We like people more after doing them a favor. Counterintuitively, asking someone for help makes them like you more, not less.",
    "why": "Cognitive dissonance: if we help someone, we rationalize that we must like them (otherwise why would we help?). Helping also creates a sense of investment and connection. We want our actions to align with our attitudes.",
    "counter": "Use this ethically to build relationships: ask for small favors. When someone helps you, recognize they may now feel more positively toward you. Don't exploit this—reciprocate and show genuine appreciation.",
    "source": "core",
    "tips": [
      "Ask small favors to build relationships and liking",
      "When someone helps you, follow up to strengthen the bond",
      "Don't just do favors—also request them to build connection",
      "Remember: people justify their helpful actions by liking you more"
    ]
  },
  {
    "id": "bystander-effect",
    "title": "Bystander Effect",
    "category": "social",
    "summary": "We're less likely to help someone in need when others are present. The more bystanders, the less likely any individual is to intervene. Responsibility diffuses across the group.",
    "why": "We look to others for cues about how to behave. If no one else is helping, we assume it's not an emergency or not our responsibility. We also fear embarrassment if we misinterpret the situation. Responsibility feels shared.",
    "counter": "Be aware of this effect and consciously override it. If you need help, single out a specific person rather than appealing to a crowd. If you witness an emergency, assume you're responsible and act. Don't wait for others.",
    "source": "core",
    "tips": [
      "In emergencies, assign specific people specific tasks",
      "Don't assume someone else will help—you do it",
      "If you need help, ask specific individuals, not the crowd",
      "Practice taking initiative in low-stakes situations"
    ]
  },
  {
    "id": "suggestibility",
    "title": "Suggestibility",
    "category": "memory",
    "summary": "Our memories are malleable and can be influenced by suggestions, leading questions, and external information. We can 'remember' events that never happened or misremember details.",
    "why": "Memory is reconstructive, not reproductive—we rebuild memories each time we recall them, incorporating new information. We're especially suggestible when uncertain, when the source seems authoritative, or when suggestions align with our expectations.",
    "counter": "Be skeptical of memories, especially vivid ones. Avoid leading questions when gathering information. Record important events contemporaneously. Recognize that confidence in a memory doesn't guarantee accuracy.",
    "source": "core",
    "tips": [
      "Be aware that how questions are asked influences answers",
      "Avoid leading questions when gathering information",
      "Notice when you're being primed by suggestions",
      "Use open-ended questions to reduce suggestion effects"
    ]
  },
  {
    "id": "false-memory",
    "title": "False Memory",
    "category": "memory",
    "summary": "We can remember events that never happened with complete confidence. False memories feel as real as true ones and can be detailed, emotional, and persistent.",
    "why": "Memory is constructive. Our brains fill gaps with plausible details, blend similar events, and incorporate suggestions. Imagination and memory use overlapping neural systems, so vividly imagining something can create a memory of it.",
    "counter": "Corroborate important memories with external evidence. Be humble about memory accuracy, especially for distant or emotional events. Avoid repeatedly imagining or discussing events, as this can distort them. Use contemporaneous records.",
    "source": "core",
    "tips": [
      "Be humble about memory certainty—memory is reconstructive",
      "Corroborate important memories with evidence when possible",
      "Don't reinforce false memories through repeated telling",
      "Be aware that vivid ≠ accurate in memory"
    ]
  },
  {
    "id": "cryptomnesia",
    "title": "Cryptomnesia",
    "category": "memory",
    "summary": "We remember information but forget where it came from, leading us to believe we came up with it ourselves. We unconsciously plagiarize, thinking others' ideas are our own original thoughts.",
    "why": "Source memory is weaker than content memory. We remember what we learned but not how or from whom. When we later recall the information, it feels like our own insight because we've forgotten the external source.",
    "counter": "Keep careful records of sources and inspirations. When you have an idea, ask: 'Where might I have encountered this before?' Give credit generously. Recognize that most ideas are built on others' work.",
    "source": "core",
    "tips": [
      "Keep detailed notes about sources of your ideas",
      "Give credit generously, especially when uncertain about origin",
      "Ask: 'Where did I first encounter this idea?'",
      "Remember: your brain naturally integrates others' ideas as your own"
    ]
  },
  {
    "id": "clustering-illusion",
    "title": "Clustering Illusion",
    "category": "perception",
    "summary": "We see patterns in random data. Streaks, clusters, and coincidences feel meaningful and non-random, even when they're exactly what we'd expect from chance.",
    "why": "Our brains are pattern-detection machines. We evolved to spot meaningful patterns (predators, food sources), so we're hypersensitive to them. We underestimate how much clustering occurs in truly random sequences.",
    "counter": "Study probability and randomness. Ask: 'What would random data actually look like?' Remember that streaks and clusters are expected in random sequences. Test your pattern against a null hypothesis. Be skeptical of 'hot hands' and 'cold streaks.'",
    "source": "core",
    "tips": [
      "Learn about randomness—true random often includes apparent patterns",
      "Ask: 'Could this pattern occur by chance?'",
      "Use statistical tests before inferring meaning from patterns",
      "Remember: your brain is a pattern-seeking machine, sometimes too eager"
    ]
  },
  {
    "id": "pessimism-bias",
    "title": "Pessimism Bias",
    "category": "perception",
    "summary": "We overestimate the likelihood of negative outcomes and underestimate positive ones. We expect the worst and are surprised when things go well.",
    "why": "Negativity bias: our brains prioritize threats for survival. Pessimism also provides emotional protection—if we expect the worst, we're prepared and won't be disappointed. Media amplifies negative news, reinforcing pessimistic worldviews.",
    "counter": "Track your predictions and compare them to outcomes—you'll likely find you're too pessimistic. Ask: 'What's the base rate for this outcome?' Balance attention to risks with attention to opportunities. Practice gratitude and positive reframing.",
    "source": "core",
    "tips": [
      "Track predictions to see how often you overestimate bad outcomes",
      "Ask: 'What's the actual base rate of this bad outcome?'",
      "Practice gratitude to counter negativity bias",
      "Remember: our brains evolved to overweight threats"
    ]
  },
  {
    "id": "optimism-bias",
    "title": "Optimism Bias",
    "category": "perception",
    "summary": "We overestimate the likelihood of positive outcomes and underestimate negative ones, especially for ourselves. We think we're less likely than others to experience misfortune.",
    "why": "Optimism is motivating and protects mental health. We have more control over our own actions than others', so we feel we can avoid negative outcomes. Optimism also feels good and is socially rewarded.",
    "counter": "Use 'pre-mortems': imagine your project has failed and work backward to identify risks. Ask: 'What could go wrong?' Consult base rates for similar endeavors. Balance optimism with realistic planning and contingencies.",
    "source": "core",
    "tips": [
      "Use 'pre-mortem' analysis: imagine failure and work backward",
      "Ask: 'What could go wrong? What's my backup plan?'",
      "Track predictions to calibrate your optimism",
      "Remember: 'it won't happen to me' is a dangerous thought"
    ],
    "examples": [
      {
        "title": "Sydney Opera House Construction",
        "description": "The Sydney Opera House was projected to cost $7 million and be completed in 4 years (1963). It actually cost $102 million and took 14 years to complete (1973). Planners were wildly optimistic about both timeline and budget, despite similar projects historically running over. This pattern of underestimating time and cost is so common in major projects it's called the 'planning fallacy.'",
        "source": "Sydney Opera House history",
        "year": 1973,
        "category": "historical"
      },
      {
        "title": "Startup Failure Rates",
        "description": "90% of startups fail, yet surveys show 80% of entrepreneurs believe their startup will succeed. When asked about others' chances, they accurately cite the 90% failure rate, but believe they're different. This optimism bias contributes to both entrepreneurial courage and poor risk assessment.",
        "category": "business"
      },
      {
        "title": "Wedding Budget Overruns",
        "description": "Studies show 80% of couples exceed their wedding budget by an average of 50%, despite knowing that most weddings go over budget. Couples consistently believe 'we'll be different' and underestimate costs, even when warned by recently married friends. Optimism bias makes them think they'll avoid the problems others faced.",
        "category": "personal"
      }
    ]
  },
  {
    "id": "blind-spot-bias",
    "title": "Blind-Spot Bias",
    "category": "perception",
    "summary": "We recognize biases in others but fail to see them in ourselves. We believe we're more objective and less biased than average, which is itself a bias.",
    "why": "We have introspective access to our own reasoning and intentions, which feel rational and justified. We only see others' conclusions and behaviors, not their internal logic. Recognizing our own biases threatens our self-image as rational.",
    "counter": "Assume you have biases you can't see. Seek external feedback. Ask: 'If someone else did this, would I call it biased?' Study cognitive biases not to spot them in others, but to catch them in yourself. Practice intellectual humility.",
    "source": "core",
    "tips": [
      "Assume you're biased, even when you can't see how",
      "Ask others: 'What biases do you see in my thinking?'",
      "Study cognitive biases to recognize them in yourself",
      "Remember: thinking you're less biased is itself a bias"
    ]
  }
]
